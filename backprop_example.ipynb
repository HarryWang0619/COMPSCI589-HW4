{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXAMPLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use this?\n",
    "\n",
    "It's highly recommend to use the _**backprop_example.ipynb**_ file to check the correctness of two examples..\n",
    "\n",
    "I provide all solutions from the txt file in the #comment, and my code include the print the solutions. \n",
    "\n",
    "It's also okay to just run the _**example.py**_ file.\n",
    "\n",
    "Even though I include most function import from utils, neuralnetwork, etc, it's still recommend to have download all files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Propagation Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import *\n",
    "from stratified import *\n",
    "from neuralnetwork import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x): # sigmoid function\n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theta 1\n",
    "# 0.40000  0.10000  \n",
    "# 0.30000  0.20000  \n",
    "theta1 = np.array([[0.4, 0.1],[0.3,0.2]])\n",
    "# Theta 2 \n",
    "#   0.70000  0.50000  0.60000\n",
    "theta2 = np.array([0.7,0.5,0.6])\n",
    "weightlist1= [theta1,theta2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set\n",
    "# \tTraining instance 1\n",
    "# \t\tx: [0.13000]\n",
    "# \t\ty: [0.90000]\n",
    "# \tTraining instance 2\n",
    "# \t\tx: [0.42000]\n",
    "# \t\ty: [0.23000]\n",
    "# Training instance 1\n",
    "trainingcategory = {'x1':'numerical', 'y':'class_numerical'}\n",
    "trainingdata1 = np.array([0.13,0.9])\n",
    "trainingdata2 = np.array([0.42,0.23])\n",
    "inputdata1 = np.append(1,trainingdata1[0])\n",
    "inputdata2 = np.append(1,trainingdata2[0])\n",
    "exceptout1 = trainingdata1[1]\n",
    "exceptout2 = trainingdata2[1]\n",
    "lambda1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costfunction(expected_output, actual_output):\n",
    "    j = -np.multiply(expected_output,np.log(actual_output)) - np.multiply((1 - expected_output),np.log(1 - actual_output))\n",
    "    return np.sum(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardtest(inputdata,weightl,expectedout):\n",
    "    current_layer_a = inputdata\n",
    "    print('current_a at 1 is',current_layer_a)\n",
    "    current_layer_index = 0\n",
    "    alist = []\n",
    "    alist.append(current_layer_a)\n",
    "    for theta in weightl:\n",
    "        z = np.dot(theta,current_layer_a)\n",
    "        a = g(z)\n",
    "        current_layer_a = np.append(1,a) if (current_layer_index+1 != len(weightl)) else a\n",
    "        print('current_a at',current_layer_index+2,'is',current_layer_a)\n",
    "        alist.append(current_layer_a)\n",
    "        current_layer_index += 1\n",
    "    result = current_layer_a\n",
    "    print('prediction is', result)\n",
    "    print('exceptout is', expectedout)\n",
    "    print('cost is', costfunction(expectedout,result))\n",
    "    return result, costfunction(expectedout,result), alist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_a at 1 is [1.   0.13]\n",
      "current_a at 2 is [1.        0.601807  0.5807858]\n",
      "current_a at 3 is 0.7940274264318581\n",
      "prediction is 0.7940274264318581\n",
      "exceptout is 0.9\n",
      "cost is 0.36557477431084995\n"
     ]
    }
   ],
   "source": [
    "r1,j1,a1 = forwardtest(inputdata1,weightlist1,exceptout1)\n",
    "# Computing the error/cost, J, of the network\n",
    "# \tProcessing training instance 1\n",
    "# \tForward propagating the input [0.13000]\n",
    "# \t\ta1: [1.00000   0.13000]\n",
    "\n",
    "# \t\tz2: [0.41300   0.32600]\n",
    "# \t\ta2: [1.00000   0.60181   0.58079]\n",
    "\n",
    "# \t\tz3: [1.34937]\n",
    "# \t\ta3: [0.79403]\n",
    "\n",
    "# \t\tf(x): [0.79403]\n",
    "# \tPredicted output for instance 1: [0.79403]\n",
    "# \tExpected output for instance 1: [0.90000]\n",
    "# \tCost, J, associated with instance 1: 0.366"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_a at 1 is [1.   0.42]\n",
      "current_a at 2 is [1.         0.60873549 0.59483749]\n",
      "current_a at 3 is 0.7959660671522611\n",
      "prediction is 0.7959660671522611\n",
      "exceptout is 0.23\n",
      "cost is 1.2763768066887786\n"
     ]
    }
   ],
   "source": [
    "r2,j2,a2 = forwardtest(inputdata2,weightlist1,exceptout2)\n",
    "\t# Processing training instance 2\n",
    "\t# Forward propagating the input [0.42000]\n",
    "\t# \ta1: [1.00000   0.42000]\n",
    "\n",
    "\t# \tz2: [0.44200   0.38400]\n",
    "\t# \ta2: [1.00000   0.60874   0.59484]\n",
    "\n",
    "\t# \tz3: [1.36127]\n",
    "\t# \ta3: [0.79597]\n",
    "\n",
    "\t# \tf(x): [0.79597]\n",
    "\t# Predicted output for instance 2: [0.79597]\n",
    "\t# Expected output for instance 2: [0.23000]\n",
    "\t# Cost, J, associated with instance 2: 1.276"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "jlist1 = np.array([j1,j2])\n",
    "numberofinstance1 = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overallcost(jlist,n,weightl,lambda_reg):\n",
    "    s = sumofweights(weightl,bias=0)*lambda_reg/(2*n)\n",
    "    jsum = np.sum(jlist)\n",
    "    return jsum/n + s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8209757904998143"
      ]
     },
     "execution_count": 582,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overallcost(jlist1,numberofinstance1,weightlist1,lambda1)\n",
    "# Final (regularized) cost, J, based on the complete training set: 0.82098"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Propagate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta(weightl,alist,expect,actual):\n",
    "    delta_layer_n = actual-expect\n",
    "    deltalist = []\n",
    "    deltalist.append(delta_layer_n)\n",
    "    i = len(weightl)-1\n",
    "    current_delta = delta_layer_n\n",
    "    while i > 0:\n",
    "        delta_layer_now = np.multiply(np.multiply(np.dot(weightl[i].T,current_delta),alist[i]),(1-alist[i]))\n",
    "        current_delta = delta_layer_now[1:]\n",
    "        deltalist.append(current_delta)\n",
    "        i-=1\n",
    "    deltalist.reverse()\n",
    "    return deltalist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientD(weights_list,delta_list,a_list,biasterm=True):\n",
    "    gradlist = []\n",
    "    for i in range(len(weights_list)):\n",
    "        anow = a_list[i]\n",
    "        deltanow = np.array([delta_list[i]]).T\n",
    "        dotproduct = deltanow*anow\n",
    "        # print('dotshape',dotproduct.shape)\n",
    "        gradlist.append(dotproduct)\n",
    "    return gradlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-0.01269739, -0.01548092]), -0.10597257356814194]\n"
     ]
    }
   ],
   "source": [
    "delta1_1 = delta(weightlist1,a1,exceptout1,r1)\n",
    "\t# Computing gradients based on training instance 1\n",
    "\t# \tdelta3: [-0.10597]\n",
    "\t# \tdelta2: [-0.01270   -0.01548]\n",
    "print(delta1_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.01269739, -0.00165066],\n",
      "       [-0.01548092, -0.00201252]]), array([-0.10597257, -0.06377504, -0.06154737])]\n"
     ]
    }
   ],
   "source": [
    "\t\t# Gradients of Theta2 based on training instance 1:\n",
    "\t\t# \t-0.10597  -0.06378  -0.06155  \n",
    "\n",
    "\t\t# Gradients of Theta1 based on training instance 1:\n",
    "\t\t# \t-0.01270  -0.00165  \n",
    "\t\t# \t-0.01548  -0.00201 \n",
    "gradd1_1 = gradientD(weightlist1,delta1_1,a1)\n",
    "print(gradd1_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.06739994, 0.08184068]), 0.5659660671522612]\n"
     ]
    }
   ],
   "source": [
    "delta1_2 = delta(weightlist1,a2,exceptout2,r2)\n",
    "\t# Computing gradients based on training instance 2\n",
    "\t# \tdelta3: [0.56597]\n",
    "\t# \tdelta2: [0.06740   0.08184]\n",
    "print(delta1_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.06739994, 0.02830797],\n",
      "       [0.08184068, 0.03437309]]), array([0.56596607, 0.34452363, 0.33665784])]\n"
     ]
    }
   ],
   "source": [
    "\t\t# Gradients of Theta2 based on training instance 2:\n",
    "\t\t# \t0.56597  0.34452  0.33666  \n",
    "\n",
    "\t\t# Gradients of Theta1 based on training instance 2:\n",
    "\t\t# \t0.06740  0.02831  \n",
    "\t\t# \t0.08184  0.03437  \n",
    "gradd1_2 = gradientD(weightlist1,delta1_2,a2)\n",
    "print(gradd1_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transposelistoflist(l):\n",
    "    newlistoflist = []\n",
    "    for i in range(len(l[0])):\n",
    "        newlist = []\n",
    "        for j in range(len(l)):\n",
    "            newlist.append(l[j][i])\n",
    "        newlistoflist.append(newlist)\n",
    "    return newlistoflist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "listofgradient = [gradd1_1,gradd1_2]\n",
    "gradientP1 = [lambda1*t for t in weightlist1]\n",
    "grad_D_transpose = transposelistoflist(listofgradient)\n",
    "grad_D_sum = [np.sum(t,axis=0) for t in grad_D_transpose]\n",
    "update_gradients = []\n",
    "for i in range(len(grad_D_sum)):\n",
    "    update_gradients.append((grad_D_sum[i] + gradientP1[i])*(1/numberofinstance1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.02735127, 0.01332866],\n",
      "       [0.03317988, 0.01618028]]), array([0.22999675, 0.1403743 , 0.13755523])]\n"
     ]
    }
   ],
   "source": [
    "print(update_gradients)\n",
    "\t# The entire training set has been processes. Computing the average (regularized) gradients:\n",
    "\t# \tFinal regularized gradients of Theta1:\n",
    "\t# \t\t0.02735  0.01333  \n",
    "\t# \t\t0.03318  0.01618  \n",
    "\n",
    "\t# \tFinal regularized gradients of Theta2:\n",
    "\t# \t\t0.23000  0.14037  0.13756  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Propagation Example 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Theta1 (the weights of each neuron, including the bias weight, are stored in the rows):\n",
    "# \t0.42000  0.15000  0.40000  \n",
    "# \t0.72000  0.10000  0.54000  \n",
    "# \t0.01000  0.19000  0.42000  \n",
    "# \t0.30000  0.35000  0.68000  \n",
    "\n",
    "# Initial Theta2 (the weights of each neuron, including the bias weight, are stored in the rows):\n",
    "# \t0.21000  0.67000  0.14000  0.96000  0.87000  \n",
    "# \t0.87000  0.42000  0.20000  0.32000  0.89000  \n",
    "# \t0.03000  0.56000  0.80000  0.69000  0.09000  \n",
    "\n",
    "# Initial Theta3 (the weights of each neuron, including the bias weight, are stored in the rows):\n",
    "# \t0.04000  0.87000  0.42000  0.53000  \n",
    "# \t0.17000  0.10000  0.95000  0.69000  \n",
    "e2theta1 = np.array([[0.42,0.15,0.4],[0.72,0.1,0.54],[0.01,0.19,0.42],[0.3,0.35,0.68]])\n",
    "e2theta2 = np.array([[0.21,0.67,0.14,0.96,0.87],[0.87,0.42,0.2,0.32,0.89],[0.03,0.56,0.8,0.69,0.09]])\n",
    "e2theta3 = np.array([[0.04,0.87,0.42,0.53],[0.17,0.1,0.95,0.69]])\n",
    "e2weightlist = [e2theta1,e2theta2,e2theta3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set\n",
    "# \tTraining instance 1\n",
    "# \t\tx: [0.32000   0.68000]\n",
    "# \t\ty: [0.75000   0.98000]\n",
    "# \tTraining instance 2\n",
    "# \t\tx: [0.83000   0.02000]\n",
    "# \t\ty: [0.75000   0.28000]\n",
    "\n",
    "e2input1 = np.array([0.32,0.68])\n",
    "e2input2 = np.array([0.83,0.02])\n",
    "e2exceptout1 = np.array([0.75,0.98])\n",
    "e2exceptout2 = np.array([0.75,0.28])\n",
    "\n",
    "e2input1 = np.append(1,e2input1)\n",
    "e2input2 = np.append(1,e2input2)\n",
    "e2lambda0 = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_a at 1 is [1.   0.32 0.68]\n",
      "current_a at 2 is [1.         0.67699586 0.75384029 0.5881687  0.70566042]\n",
      "current_a at 3 is [1.         0.87519469 0.89296181 0.81480444]\n",
      "current_a at 4 is [0.83317658 0.84131543]\n",
      "prediction is [0.83317658 0.84131543]\n",
      "exceptout is [0.75 0.98]\n",
      "cost is 0.7907366961135718\n"
     ]
    }
   ],
   "source": [
    "e2r1,e2j1,e2a1 = forwardtest(e2input1,e2weightlist,e2exceptout1)\n",
    "\t# Processing training instance 1\n",
    "\t# Forward propagating the input [0.32000   0.68000]\n",
    "\t# \ta1: [1.00000   0.32000   0.68000]\n",
    "\n",
    "\t# \tz2: [0.74000   1.11920   0.35640   0.87440]\n",
    "\t# \ta2: [1.00000   0.67700   0.75384   0.58817   0.70566]\n",
    "\n",
    "\t# \tz3: [1.94769   2.12136   1.48154]\n",
    "\t# \ta3: [1.00000   0.87519   0.89296   0.81480]\n",
    "\n",
    "\t# \tz4: [1.60831   1.66805]\n",
    "\t# \ta4: [0.83318   0.84132]\n",
    "\n",
    "\t# \tf(x): [0.83318   0.84132]\n",
    "\t# Predicted output for instance 1: [0.83318   0.84132]\n",
    "\t# Expected output for instance 1: [0.75000   0.98000]\n",
    "\t# Cost, J, associated with instance 1: 0.791"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_a at 1 is [1.   0.83 0.02]\n",
      "current_a at 2 is [1.         0.63471542 0.69291867 0.54391158 0.64659376]\n",
      "current_a at 3 is [1.         0.86020091 0.88336451 0.79790763]\n",
      "current_a at 4 is [0.82952703 0.83831889]\n",
      "prediction is [0.82952703 0.83831889]\n",
      "exceptout is [0.75 0.28]\n",
      "cost is 1.9437823352945296\n"
     ]
    }
   ],
   "source": [
    "e2r2,e2j2,e2a2 = forwardtest(e2input2,e2weightlist,e2exceptout2)\n",
    "\t# Processing training instance 2\n",
    "\t# Forward propagating the input [0.83000   0.02000]\n",
    "\t# \ta1: [1.00000   0.83000   0.02000]\n",
    "\n",
    "\t# \tz2: [0.55250   0.81380   0.17610   0.60410]\n",
    "\t# \ta2: [1.00000   0.63472   0.69292   0.54391   0.64659]\n",
    "\n",
    "\t# \tz3: [1.81696   2.02468   1.37327]\n",
    "\t# \ta3: [1.00000   0.86020   0.88336   0.79791]\n",
    "\n",
    "\t# \tz4: [1.58228   1.64577]\n",
    "\t# \ta4: [0.82953   0.83832]\n",
    "\n",
    "\t# \tf(x): [0.82953   0.83832]\n",
    "\t# Predicted output for instance 2: [0.82953   0.83832]\n",
    "\t# Expected output for instance 2: [0.75000   0.28000]\n",
    "\t# Cost, J, associated with instance 2: 1.944"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "e2jlist = np.array([e2j1,e2j2])\n",
    "e2numberofinstance = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sumofweights(listofweights,bias=True): # computes the square of all weights of the network and sum them up\n",
    "    sum = 0\n",
    "    for weight in listofweights:\n",
    "        if bias:\n",
    "            w = weight.copy()\n",
    "            w[:, 0] = 0\n",
    "            sum += np.sum(np.square(w))\n",
    "        else:\n",
    "            sum += np.sum(np.square(weight))\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overallcost(jlist,n,weightl,lambda_reg):\n",
    "    s = sumofweights(weightl,bias=1)*lambda_reg/(2*n)\n",
    "    jsum = np.sum(jlist)\n",
    "    return jsum/n + s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9035095157040507"
      ]
     },
     "execution_count": 599,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overallcost(e2jlist,e2numberofinstance,e2weightlist,e2lambda0)\n",
    "# Final (regularized) cost, J, based on the complete training set: 1.90351"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Propagation for E2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-0.00086743, -0.00133354, -0.00053312, -0.00070163]), array([ 0.00638937, -0.00925379, -0.00778767]), array([ 0.08317658, -0.13868457])]\n"
     ]
    }
   ],
   "source": [
    "e2delta1 = delta(e2weightlist,e2a1,e2exceptout1,e2r1)\n",
    "# Running backpropagation\n",
    "# \tComputing gradients based on training instance 1\n",
    "# \t\tdelta4: [0.08318   -0.13868]\n",
    "# \t\tdelta3: [0.00639   -0.00925   -0.00779]\n",
    "# \t\tdelta2: [-0.00087   -0.00133   -0.00053   -0.00070]\n",
    "print(e2delta1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.00086743, -0.00027758, -0.00058985],\n",
      "       [-0.00133354, -0.00042673, -0.00090681],\n",
      "       [-0.00053312, -0.0001706 , -0.00036252],\n",
      "       [-0.00070163, -0.00022452, -0.00047711]]), array([[ 0.00638937,  0.00432557,  0.00481656,  0.00375802,  0.00450872],\n",
      "       [-0.00925379, -0.00626478, -0.00697588, -0.00544279, -0.00653003],\n",
      "       [-0.00778767, -0.00527222, -0.00587066, -0.00458046, -0.00549545]]), array([[ 0.08317658,  0.0727957 ,  0.07427351,  0.06777264],\n",
      "       [-0.13868457, -0.121376  , -0.12384003, -0.1130008 ]])]\n"
     ]
    }
   ],
   "source": [
    "\t\t# Gradients of Theta3 based on training instance 1:\n",
    "\t\t# \t0.08318  0.07280  0.07427  0.06777  \n",
    "\t\t# \t-0.13868  -0.12138  -0.12384  -0.11300  \n",
    "\n",
    "\t\t# Gradients of Theta2 based on training instance 1:\n",
    "\t\t# \t0.00639  0.00433  0.00482  0.00376  0.00451  \n",
    "\t\t# \t-0.00925  -0.00626  -0.00698  -0.00544  -0.00653  \n",
    "\t\t# \t-0.00779  -0.00527  -0.00587  -0.00458  -0.00550  \n",
    "\n",
    "\t\t# Gradients of Theta1 based on training instance 1:\n",
    "\t\t# \t-0.00087  -0.00028  -0.00059  \n",
    "\t\t# \t-0.00133  -0.00043  -0.00091  \n",
    "\t\t# \t-0.00053  -0.00017  -0.00036  \n",
    "\t\t# \t-0.00070  -0.00022  -0.00048  \n",
    "e2grad1 = gradientD(e2weightlist,e2delta1,e2a1)\n",
    "print(e2grad1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.01694006, 0.01465141, 0.01998824, 0.01622017]), array([0.01503437, 0.05808969, 0.06891698]), array([0.07952703, 0.55831889])]\n"
     ]
    }
   ],
   "source": [
    "e2delta2 = delta(e2weightlist,e2a2,e2exceptout2,e2r2)\n",
    "\t# Computing gradients based on training instance 2\n",
    "\t# \tdelta4: [0.07953   0.55832]\n",
    "\t# \tdelta3: [0.01503   0.05809   0.06892]\n",
    "\t# \tdelta2: [0.01694   0.01465   0.01999   0.01622]\n",
    "print(e2delta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.01694006, 0.01406025, 0.0003388 ],\n",
      "       [0.01465141, 0.01216067, 0.00029303],\n",
      "       [0.01998824, 0.01659024, 0.00039976],\n",
      "       [0.01622017, 0.01346274, 0.0003244 ]]), array([[0.01503437, 0.00954254, 0.01041759, 0.00817737, 0.00972113],\n",
      "       [0.05808969, 0.03687042, 0.04025143, 0.03159565, 0.03756043],\n",
      "       [0.06891698, 0.04374267, 0.04775386, 0.03748474, 0.04456129]]), array([[0.07952703, 0.06840922, 0.07025135, 0.06345522],\n",
      "       [0.55831889, 0.48026642, 0.4931991 , 0.44548691]])]\n"
     ]
    }
   ],
   "source": [
    "\t\t# Gradients of Theta3 based on training instance 2:\n",
    "\t\t# \t0.07953  0.06841  0.07025  0.06346  \n",
    "\t\t# \t0.55832  0.48027  0.49320  0.44549  \n",
    "\n",
    "\t\t# Gradients of Theta2 based on training instance 2:\n",
    "\t\t# \t0.01503  0.00954  0.01042  0.00818  0.00972  \n",
    "\t\t# \t0.05809  0.03687  0.04025  0.03160  0.03756  \n",
    "\t\t# \t0.06892  0.04374  0.04775  0.03748  0.04456  \n",
    "\n",
    "\t\t# Gradients of Theta1 based on training instance 2:\n",
    "\t\t# \t0.01694  0.01406  0.00034  \n",
    "\t\t# \t0.01465  0.01216  0.00029  \n",
    "\t\t# \t0.01999  0.01659  0.00040  \n",
    "\t\t# \t0.01622  0.01346  0.00032  \n",
    "e2grad2 = gradientD(e2weightlist,e2delta2,e2a2)\n",
    "print(e2grad2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "e2listofgradient = [e2grad1,e2grad2]\n",
    "gradientP2 = [e2lambda0*t for t in e2weightlist]\n",
    "for singleP in gradientP2:\n",
    "    singleP[:, 0] = 0       \n",
    "e2_grad_D_transpose = transposelistoflist(e2listofgradient)\n",
    "e2_grad_D_sum = [np.sum(t,axis=0) for t in e2_grad_D_transpose]\n",
    "e2_update_gradients = []\n",
    "for i in range(len(grad_D_sum)):\n",
    "    e2_update_gradients.append((e2_grad_D_sum[i] + gradientP2[i])*(1/e2numberofinstance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.00803632, 0.02564134, 0.04987447],\n",
      "       [0.00665894, 0.01836697, 0.06719311],\n",
      "       [0.00972756, 0.03195982, 0.05251862],\n",
      "       [0.00775927, 0.05036911, 0.08492365]]), array([[0.01071187, 0.09068406, 0.02511708, 0.1259677 , 0.11586492],\n",
      "       [0.02441795, 0.06780282, 0.04163777, 0.05307643, 0.1267652 ],\n",
      "       [0.03056466, 0.08923522, 0.1209416 , 0.10270214, 0.03078292]])]\n"
     ]
    }
   ],
   "source": [
    "print(e2_update_gradients)\n",
    "\t# The entire training set has been processes. Computing the average (regularized) gradients:\n",
    "\t# \tFinal regularized gradients of Theta1:\n",
    "\t# \t\t0.00804  0.02564  0.04987  \n",
    "\t# \t\t0.00666  0.01837  0.06719  \n",
    "\t# \t\t0.00973  0.03196  0.05252  \n",
    "\t# \t\t0.00776  0.05037  0.08492  \n",
    "\n",
    "\t# \tFinal regularized gradients of Theta2:\n",
    "\t# \t\t0.01071  0.09068  0.02512  0.12597  0.11586  \n",
    "\t# \t\t0.02442  0.06780  0.04164  0.05308  0.12677  \n",
    "\t# \t\t0.03056  0.08924  0.12094  0.10270  0.03078  \n",
    "\n",
    "\t# \tFinal regularized gradients of Theta3:\n",
    "\t# \t\t0.08135  0.17935  0.12476  0.13186  \n",
    "\t# \t\t0.20982  0.19195  0.30343  0.25249  "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
